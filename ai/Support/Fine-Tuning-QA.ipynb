{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning for QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, T5Tokenizer, T5ForConditionalGeneration, GenerationConfig, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n",
      "4.36.2\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import transformers\n",
    "print(datasets.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define config parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name='google/flan-t5-small'\n",
    "model_name='t5-small'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnmoses/miniforge3/envs/mforge39/lib/python3.9/site-packages/huggingface_hub-0.24.0-py3.8.egg/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "base_model = base_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_bitext_train = load_dataset(\"bitext/Bitext-customer-support-llm-chatbot-training-dataset\", split=\"train[:16000]\")\n",
    "dataset_bitext_test = load_dataset(\"bitext/Bitext-customer-support-llm-chatbot-training-dataset\", split=\"train[-4000:-2000]\")\n",
    "dataset_bitext_validation = load_dataset(\"bitext/Bitext-customer-support-llm-chatbot-training-dataset\", split=\"train[-2000:]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6055cb062784938936dbe79e9b3de3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnmoses/miniforge3/envs/mforge39/lib/python3.9/site-packages/datasets/download/streaming_download_manager.py:778: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['flags', 'instruction', 'category', 'intent', 'response'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_local_train = load_dataset('csv', split='train[:80%]', data_files={'local_merged.csv'})\n",
    "dataset_local_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['flags', 'instruction', 'category', 'intent', 'response'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_local_test = load_dataset('csv', split='train[-20%:-10%]', data_files={'local_merged.csv'})\n",
    "dataset_local_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['flags', 'instruction', 'category', 'intent', 'response'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_local_validation = load_dataset('csv', split='train[-10%:]', data_files={'local_merged.csv'})\n",
    "dataset_local_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_merged = concatenate_datasets(\n",
    "    [\n",
    "        dataset_bitext_train, \n",
    "        dataset_local_train\n",
    "    ]\n",
    ")\n",
    "dataset_test_merged = concatenate_datasets(\n",
    "    [\n",
    "        dataset_bitext_test, \n",
    "        dataset_local_test\n",
    "    ]\n",
    ")\n",
    "dataset_validation_merged = concatenate_datasets(\n",
    "    [\n",
    "        dataset_bitext_validation,\n",
    "        dataset_local_validation\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87845989f8c24b99aa30dbe0a546a7fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/17 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6b177f7c5c4e07824800d28d29bab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed18e0fec7c4b708ac7a84a69c07200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1109899"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_merged.to_csv('train_merged.csv', index=False)\n",
    "dataset_test_merged.to_csv('test_merged.csv', index=False)\n",
    "dataset_validation_merged.to_csv('validation_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5855755f51f84cd4b3c8a9dd7316c20c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "255da8500e234847b91e40b8457ffbc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5181d2d89f944d3baebf8efb7ef7934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files={\n",
    "    \"train\": \"train_merged.csv\", \n",
    "    \"test\": \"test_merged.csv\", \n",
    "    \"validation\": \"validation_merged.csv\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['flags', 'instruction', 'category', 'intent', 'response'],\n",
       "        num_rows: 16010\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['flags', 'instruction', 'category', 'intent', 'response'],\n",
       "        num_rows: 2001\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['flags', 'instruction', 'category', 'intent', 'response'],\n",
       "        num_rows: 2001\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'flags': 'B',\n",
       " 'instruction': 'question about cancelling order {{Order Number}}',\n",
       " 'category': 'ORDER',\n",
       " 'intent': 'cancel_order',\n",
       " 'response': \"I've understood you have a question regarding canceling order {{Order Number}}, and I'm here to provide you with the information you need. Please go ahead and ask your question, and I'll do my best to assist you.\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = pd.read_csv('./train_merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flags</th>\n",
       "      <th>instruction</th>\n",
       "      <th>category</th>\n",
       "      <th>intent</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>question about cancelling order {{Order Number}}</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>I've understood you have a question regarding ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BQZ</td>\n",
       "      <td>i have a question about cancelling oorder {{Or...</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>I've been informed that you have a question ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BLQZ</td>\n",
       "      <td>i need help cancelling puchase {{Order Number}}</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>I can sense that you're seeking assistance wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BL</td>\n",
       "      <td>I need to cancel purchase {{Order Number}}</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>I understood that you need assistance with can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BCELN</td>\n",
       "      <td>I cannot afford this order, cancel purchase {{...</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>I'm sensitive to the fact that you're facing f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flags                                        instruction category  \\\n",
       "0      B   question about cancelling order {{Order Number}}    ORDER   \n",
       "1    BQZ  i have a question about cancelling oorder {{Or...    ORDER   \n",
       "2   BLQZ    i need help cancelling puchase {{Order Number}}    ORDER   \n",
       "3     BL         I need to cancel purchase {{Order Number}}    ORDER   \n",
       "4  BCELN  I cannot afford this order, cancel purchase {{...    ORDER   \n",
       "\n",
       "         intent                                           response  \n",
       "0  cancel_order  I've understood you have a question regarding ...  \n",
       "1  cancel_order  I've been informed that you have a question ab...  \n",
       "2  cancel_order  I can sense that you're seeking assistance wit...  \n",
       "3  cancel_order  I understood that you need assistance with can...  \n",
       "4  cancel_order  I'm sensitive to the fact that you're facing f...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, eos_token=\"<|im_end|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHAT_ML_TEMPLATE = \"\"\"\n",
    "# {% for message in messages %}\n",
    "#     {% if message['role'] == 'user' %}\n",
    "#         {{'<|im_start|>user\\n' + message['content'].strip() + '<|im_end|>' }}\n",
    "#     {% elif message['role'] == 'system' %}\n",
    "#         {{'<|im_start|>system\\n' + message['content'].strip() + '<|im_end|>' }}\n",
    "#     {% elif message['role'] == 'assistant' %}\n",
    "#         {{'<|im_start|>assistant\\n'  + message['content'] + '<|im_end|>' }}\n",
    "#     {% endif %}\n",
    "# {% endfor %}\n",
    "# \"\"\"\n",
    "# tokenizer.chat_template = CHAT_ML_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template = \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHAT_ML_TEMPLATE = \"\"\" \n",
    "# {%- for message in messages %}\n",
    "#     {%- if message['role'] == 'user' %}\n",
    "#         {{- bos_token + '[INST] ' + message['content'].strip() + ' [/INST]' }}\n",
    "#     {%- elif message['role'] == 'system' %}\n",
    "#         {{- '<<SYS>>\\\\n' + message['content'].strip() + '\\\\n<</SYS>>\\\\n\\\\n' }}\n",
    "#     {%- elif message['role'] == 'assistant' %}\n",
    "#         {{- '[ASST] '  + message['content'] + ' [/ASST]' + eos_token }}\n",
    "#     {%- endif %}\n",
    "# {%- endfor %}\n",
    "# \"\"\"\n",
    "\n",
    "# template = tokenizer.chat_template\n",
    "# template = template.replace(\"SYS\", \"SYSTEM\")  # Change the system token\n",
    "# tokenizer.chat_template = template  # Set the new template\n",
    "# tokenizer.push_to_hub(\"model_name\")  # Upload your new template to the Hub!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|im_start|>user\\nHello, how are you?<|im_end|>\\n<|im_start|>assistant\\nI'm doing great. How can I help you today?<|im_end|>\\n<|im_start|>user\\nI'd like to show off how chat templating works!<|im_end|>\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = [\n",
    "  {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "  {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n",
    "]\n",
    "\n",
    "# tokenizer.use_default_system_prompt = False\n",
    "tokenizer.apply_chat_template(chat, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "<pad> Wie alt sind Sie?</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnmoses/miniconda3/envs/mconda38/lib/python3.8/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_text = \"translate English to German: How old are you?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = base_model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(data, index):\n",
    "    instruction = data.iloc[index]['instruction']\n",
    "    response = data.iloc[index]['response']\n",
    "    \n",
    "    prompt = f'''\n",
    "    instruction:\n",
    "    {instruction}\n",
    "    \n",
    "    response:\n",
    "    {response}\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def get_instruction(data,index):\n",
    "    instruction = data.iloc[index]['instruction']\n",
    "    return f'''\n",
    "    \n",
    "    instruction:\n",
    "    {instruction}\n",
    "    \n",
    "    response:\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    instruction:\n",
      "    question about cancelling order {{Order Number}}\n",
      "    \n",
      "    response:\n",
      "    I've understood you have a question regarding canceling order {{Order Number}}, and I'm here to provide you with the information you need. Please go ahead and ask your question, and I'll do my best to assist you.\n",
      "    \n",
      "    \n",
      "    \n",
      "    instruction:\n",
      "    i dont know what to do to cancel order {{Order Number}}\n",
      "    \n",
      "    response:\n",
      "    \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "test_prompt = get_prompt(dataset_train,0) + get_instruction(dataset_train,10)\n",
    "print(test_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad><extra_id_0>: I have understood you have a question regarding cancelling order <unk>Order Number\n"
     ]
    }
   ],
   "source": [
    "input_text = tokenizer.encode(test_prompt,return_tensors=\"pt\")\n",
    "output = tokenizer.decode(base_model.generate(input_text)[0])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction:\n",
      "canceling order {{Order Number}}\n",
      "\n",
      "response:\n",
      "\n",
      "======================\n",
      "expected reponse: \n",
      " I appreciate that you're seeking assistance with canceling your order tied to the order number {{Order Number}}, and I'm here to guide you through the cancellation process. Please follow these steps:\n",
      "\n",
      "1. Access Your Account: Log in to your {{Online Company Portal Info}} using your username and password.\n",
      "2. Navigate to Your Orders: Once you're logged in, locate the '{{Online Order Interaction}}' or '{{Online Order Interaction}}' section.\n",
      "3. Locate the Order: Look for the specific order associated with the order number {{Order Number}}.\n",
      "4. Initiate the Cancellation: Within the order details, you should find an option to '{{Online Order Interaction}}'. Click on it to start the cancellation process.\n",
      "5. Confirm the Cancellation: The system may ask for your confirmation or gather feedback. Please provide the necessary information accurately.\n",
      "\n",
      "If you encounter any difficulties or have further questions, our dedicated customer support team is available during {{Customer Support Hours}}. You can reach us at {{Customer Support Phone Number}} or through the Live Chat feature on our {{Website URL}}. We value your satisfaction and are committed to assisting you through this process.\n"
     ]
    }
   ],
   "source": [
    "idx = 100\n",
    "idx_var = 50\n",
    "\n",
    "check = dataset_train.iloc[idx]['instruction']\n",
    "check2 = dataset_train.iloc[idx]['response']\n",
    "\n",
    "test = dataset_train.iloc[idx_var]['instruction']\n",
    "test_response = dataset_train.iloc[idx_var]['response']\n",
    "\n",
    "print(f'instruction:\\n{test}\\n\\nresponse:\\n')\n",
    "\n",
    "print(f'======================\\nexpected reponse: \\n {test_response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Instruction:\\n'\n",
    "    end_prompt = '\\nResponse:'\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"instruction\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"response\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset = dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f2872b39164f56851c088cb924e2f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16010 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d48cfbbbf5b470b95659b67b5610963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2001 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5489205b2e6f406195929ec2a8e74133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2001 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = shuffled_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['flags', 'instruction', 'category', 'intent', 'response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 16010\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 2001\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 2001\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "finetuned_model = finetuned_model.to('cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnmoses/miniconda3/envs/mconda38/lib/python3.8/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'training-t5'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=5e-3,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=16,     # batch size per device during training\n",
    "    per_device_eval_batch_size=16,      # batch size for evaluation\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy='steps',        # evaluation strategy to adopt during training\n",
    "    eval_steps=500,                 \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=finetuned_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc0514c737d4711aef1cef441a1662b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.732622742652893,\n",
       " 'eval_model_preparation_time': 0.0019,\n",
       " 'eval_runtime': 78.5127,\n",
       " 'eval_samples_per_second': 25.486,\n",
       " 'eval_steps_per_second': 1.605}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82350b07380f4e41868cda81ea3fd6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2002 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0416, 'grad_norm': 0.12597836554050446, 'learning_rate': 0.004875124875124875, 'epoch': 0.05}\n",
      "{'loss': 0.4333, 'grad_norm': 0.09609200805425644, 'learning_rate': 0.00475024975024975, 'epoch': 0.1}\n",
      "{'loss': 0.3822, 'grad_norm': 0.0963071659207344, 'learning_rate': 0.004625374625374625, 'epoch': 0.15}\n",
      "{'loss': 0.3633, 'grad_norm': 0.12200659513473511, 'learning_rate': 0.004500499500499501, 'epoch': 0.2}\n",
      "{'loss': 0.3547, 'grad_norm': 0.10900302231311798, 'learning_rate': 0.004375624375624376, 'epoch': 0.25}\n",
      "{'loss': 0.3325, 'grad_norm': 0.13998627662658691, 'learning_rate': 0.004250749250749251, 'epoch': 0.3}\n",
      "{'loss': 0.3285, 'grad_norm': 0.22718411684036255, 'learning_rate': 0.004125874125874126, 'epoch': 0.35}\n",
      "{'loss': 0.3063, 'grad_norm': 0.12131989002227783, 'learning_rate': 0.004000999000999001, 'epoch': 0.4}\n",
      "{'loss': 0.2987, 'grad_norm': 0.11856796592473984, 'learning_rate': 0.003876123876123876, 'epoch': 0.45}\n",
      "{'loss': 0.2952, 'grad_norm': 0.0806303396821022, 'learning_rate': 0.0037512487512487516, 'epoch': 0.5}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b551bc08554dddb9277ec459249393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31155863404273987, 'eval_model_preparation_time': 0.0019, 'eval_runtime': 76.8942, 'eval_samples_per_second': 26.023, 'eval_steps_per_second': 1.639, 'epoch': 0.5}\n",
      "{'loss': 0.2889, 'grad_norm': 0.08306342363357544, 'learning_rate': 0.003626373626373626, 'epoch': 0.55}\n",
      "{'loss': 0.2761, 'grad_norm': 0.07841289043426514, 'learning_rate': 0.0035014985014985016, 'epoch': 0.6}\n",
      "{'loss': 0.267, 'grad_norm': 0.09067802131175995, 'learning_rate': 0.0033766233766233766, 'epoch': 0.65}\n",
      "{'loss': 0.2741, 'grad_norm': 0.10285599529743195, 'learning_rate': 0.0032517482517482516, 'epoch': 0.7}\n",
      "{'loss': 0.2633, 'grad_norm': 0.08768812566995621, 'learning_rate': 0.003126873126873127, 'epoch': 0.75}\n",
      "{'loss': 0.2688, 'grad_norm': 0.076161690056324, 'learning_rate': 0.003001998001998002, 'epoch': 0.8}\n",
      "{'loss': 0.2602, 'grad_norm': 0.10809271782636642, 'learning_rate': 0.0028771228771228776, 'epoch': 0.85}\n",
      "{'loss': 0.2584, 'grad_norm': 0.09937774389982224, 'learning_rate': 0.0027522477522477526, 'epoch': 0.9}\n",
      "{'loss': 0.2504, 'grad_norm': 0.07943037152290344, 'learning_rate': 0.002627372627372627, 'epoch': 0.95}\n",
      "{'loss': 0.2605, 'grad_norm': 0.08241681754589081, 'learning_rate': 0.0025024975024975026, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0e738f352e42b6b313c98582d720dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2808082699775696, 'eval_model_preparation_time': 0.0019, 'eval_runtime': 76.5582, 'eval_samples_per_second': 26.137, 'eval_steps_per_second': 1.646, 'epoch': 1.0}\n",
      "{'loss': 0.2336, 'grad_norm': 0.08823828399181366, 'learning_rate': 0.0023776223776223776, 'epoch': 1.05}\n",
      "{'loss': 0.2282, 'grad_norm': 0.09146828204393387, 'learning_rate': 0.0022527472527472526, 'epoch': 1.1}\n",
      "{'loss': 0.228, 'grad_norm': 0.08485770225524902, 'learning_rate': 0.002127872127872128, 'epoch': 1.15}\n",
      "{'loss': 0.2409, 'grad_norm': 0.07330963760614395, 'learning_rate': 0.002002997002997003, 'epoch': 1.2}\n",
      "{'loss': 0.2274, 'grad_norm': 0.09222166985273361, 'learning_rate': 0.0018781218781218781, 'epoch': 1.25}\n",
      "{'loss': 0.2353, 'grad_norm': 0.08574673533439636, 'learning_rate': 0.0017532467532467534, 'epoch': 1.3}\n",
      "{'loss': 0.2228, 'grad_norm': 0.08482550084590912, 'learning_rate': 0.0016283716283716286, 'epoch': 1.35}\n",
      "{'loss': 0.2273, 'grad_norm': 0.08906550705432892, 'learning_rate': 0.0015034965034965034, 'epoch': 1.4}\n",
      "{'loss': 0.2274, 'grad_norm': 0.08723798394203186, 'learning_rate': 0.0013786213786213786, 'epoch': 1.45}\n",
      "{'loss': 0.2258, 'grad_norm': 0.10378867387771606, 'learning_rate': 0.0012537462537462539, 'epoch': 1.5}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c919171c244e59a05983f4b6cfe1f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2774794399738312, 'eval_model_preparation_time': 0.0019, 'eval_runtime': 77.862, 'eval_samples_per_second': 25.699, 'eval_steps_per_second': 1.618, 'epoch': 1.5}\n",
      "{'loss': 0.2156, 'grad_norm': 0.06207076460123062, 'learning_rate': 0.0011288711288711289, 'epoch': 1.55}\n",
      "{'loss': 0.2277, 'grad_norm': 0.08566132932901382, 'learning_rate': 0.001003996003996004, 'epoch': 1.6}\n",
      "{'loss': 0.2222, 'grad_norm': 0.08713541179895401, 'learning_rate': 0.0008791208791208792, 'epoch': 1.65}\n",
      "{'loss': 0.2143, 'grad_norm': 0.07805190235376358, 'learning_rate': 0.0007542457542457542, 'epoch': 1.7}\n",
      "{'loss': 0.2195, 'grad_norm': 0.07622843235731125, 'learning_rate': 0.0006293706293706295, 'epoch': 1.75}\n",
      "{'loss': 0.2125, 'grad_norm': 0.06771530956029892, 'learning_rate': 0.0005044955044955045, 'epoch': 1.8}\n",
      "{'loss': 0.2134, 'grad_norm': 0.08639439940452576, 'learning_rate': 0.0003796203796203796, 'epoch': 1.85}\n",
      "{'loss': 0.217, 'grad_norm': 0.06294690817594528, 'learning_rate': 0.00025474525474525475, 'epoch': 1.9}\n",
      "{'loss': 0.2114, 'grad_norm': 0.07093291729688644, 'learning_rate': 0.00012987012987012987, 'epoch': 1.95}\n",
      "{'loss': 0.2267, 'grad_norm': 0.08697705715894699, 'learning_rate': 4.995004995004995e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08cdb4c2afc4a7896689c3bb10942de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2703797519207001, 'eval_model_preparation_time': 0.0019, 'eval_runtime': 77.3108, 'eval_samples_per_second': 25.883, 'eval_steps_per_second': 1.63, 'epoch': 2.0}\n",
      "{'train_runtime': 21531.7692, 'train_samples_per_second': 1.487, 'train_steps_per_second': 0.093, 'train_loss': 0.2819580896780803, 'epoch': 2.0}\n",
      "CPU times: user 19min 17s, sys: 55min 14s, total: 1h 14min 32s\n",
      "Wall time: 5h 58min 51s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2002, training_loss=0.2819580896780803, metrics={'train_runtime': 21531.7692, 'train_samples_per_second': 1.487, 'train_steps_per_second': 0.093, 'total_flos': 4333644483133440.0, 'train_loss': 0.2819580896780803, 'epoch': 2.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model.save_pretrained(\"model-t5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model-t5/tokenizer_config.json',\n",
       " 'model-t5/special_tokens_map.json',\n",
       " 'model-t5/tokenizer.json')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"model-t5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"model-t5\"\n",
    "finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "finetuned_model = finetuned_model.to('cpu')\n",
    "\n",
    "finetuned_tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_sample(example):\n",
    "    \n",
    "    processed_example = \"<|system|>\\n You are a support chatbot who helps with user queries chatbot who always responds in the style of a professional.</s>\\n<|user|>\\n\" + example[\"instruction\"] + \"</s>\\n<|response|>\\n\"\n",
    "\n",
    "    return processed_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(question):\n",
    "    input_str = process_data_sample(\n",
    "        {\n",
    "            \"instruction\": question,\n",
    "        }\n",
    "    )\n",
    "    inputs = finetuned_tokenizer(input_str, return_tensors=\"pt\").to(device)\n",
    "    outputs = finetuned_model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        top_k=1,\n",
    "        temperature=0.1,\n",
    "        max_new_tokens=256,\n",
    "        pad_token_id=finetuned_tokenizer.eos_token_id\n",
    "        )\n",
    "    return finetuned_tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I've realized you have a question regarding canceling order Order Number. I apologize for any inconvenience this may have caused you. I apologize for any inconvenience this may have caused you. I apologize for any inconvenience this may have caused you. I apologize for any inconvenience this may have caused you. I understand that you have a question regarding canceling order Order Number and I'm here to assist you. Please feel free to ask, and I'll be more than happy to assist you. Thank you for bringing this matter to our attention, and I'm unable to cancel the order. I'm unable to assist you with canceling the order and I apologize for any inconvenience caused. Thank you for bringing this matter to our attention, and I apologize for any inconvenience caused. Thank you for bringing this matter to our attention, and I apologize for any inconvenience caused. Thank you for bringing this matter to our attention, and I'm here to assist you. Thank you for bringing this matter to our attention, and I apologize for any inconvenience caused. Thank you for bringing this matter to our attention, and I apologize for any inconvenience caused. Thank you for bringing this matter to\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"i have a question about cancelling order {{Order Number}}\"\n",
    "get_response(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: do ya have an e-mail to send a review for a service\n",
      "Predict. :I hear you! I'm following you that you're requesting assistance in a chatbot. I'm here to help you with that. You can connect with a live chat agent, and you're chatting with a live agent. We're here to help you. Please feel free to connect with a live agent, and I'll be happy to connect you with a live agent. We're always here to help you.\n",
      "Expected: Indeed! I'm thrilled to assist you with sending a review for our service. While we don't have an email specifically dedicated to receiving reviews, we have alternative methods that you can utilize to provide your feedback. One option is to visit our website and navigate to the \"Contact Us\" or \"Feedback\" page. There, you can fill out a form or send a message directly to our customer support team. Another option is to share your review on our social media platforms or review websites. We highly value your feedback and look forward to hearing about your experience! Is there anything else I can assist you with?\n",
      "=================================\n",
      "\n",
      "Instruction: where could I send feedback for your services?\n",
      "Predict. :We're here for you! I'm talking to a human agent. I'm more than happy to help you with that. You can connect with a live chat agent, and you're chatting with a live chat agent. We're here to help you. You can connect with a live chat agent, and you're chatting with a live chat agent. We're more than happy to chat with a live chat option. You can chat with a live chat option on our website. We're here to support you. We're here to support you. We're more than happy to connect you with a live chat chat. We're more than happy to connect you with a live chat option. We're here to support you, and we're chatting with a chatbot. We're a chatbot, you're chatting with a chatbot. We're more than happy to connect you with a live chat option. We're here to support you, and you're chatting with a chatbot. We're a chatbot, you're chatting with a chatbot. We're\n",
      "Expected: We greatly appreciate your willingness to provide feedback on our services! Your insights are invaluable in helping us enhance and tailor our offerings to meet your needs. To share your feedback, you can visit our website and navigate to the dedicated feedback or contact us page. There, you will find various options such as an online form, an email address, or a live chat feature, where you can conveniently submit your comments. If you have any questions or need further assistance, please feel free to let me know.\n",
      "=================================\n",
      "\n",
      "Instruction: want assistance sending some feedback about a product\n",
      "Predict. :Thank you for reaching out! I'm sorry to hear that you're having trouble contacting a product. I'm here to help you with that. We have a dedicated customer support team available to assist you. You can contact us during Customer Support Hours. We are available to assist you with any questions or concerns you may have. We're here to help! If you have any other questions or concerns, please feel free to let me know. We're here to help!\n",
      "Expected: Thanks for dropping us a line to us for assistance with sending feedback about a product. We truly value your input and appreciate your dedication to helping us improve. To ensure your feedback reaches the right team, please provide us with some details about the product you would like to leave feedback for. Is there a specific feature, functionality, or aspect of the product that you would like to highlight or comment on? Your feedback will play a critical role in shaping our future offerings and enhancing the overall experience for our valued customers like yourself.\n",
      "=================================\n",
      "\n",
      "Instruction: i have got to write an opinion for a service\n",
      "Predict. :We're here for you! I'm speaking with a human agent. I'm glad you're interacting with us. We're here to help you with that. We're more than happy to chat with a live chatbot. You're a chatbot, and you're chatting with a live chat agent. We're here to help you. We're more than happy to chat with a live chat agent. We're more than happy to chat with a live chat agent. We're here to help! We're a chat chat option, and you're chatting with a chat chat chat. We're more than happy to connect you with a live chat chat option. We're here to support you, and you're chatting with a chatbot. We're more than happy to connect with a live chat option. We're more than happy to connect you with a live chat option. We're here to help you connect with a live chat chat option. We're more than happy to connect you with a live chat chat option. We're more than happy to connect you with a live\n",
      "Expected: Thank you for reaching out! I completely understand that you need to write an opinion for a service. Your feedback is incredibly valuable to us, as it helps us improve and provide a better experience for you and our future customers. To share your opinion, you can visit our website and navigate to the \"Feedback\" or \"Contact Us\" section. There, you will find a form or an email address where you can express your thoughts and provide detailed feedback about the service you received. We highly appreciate your input and look forward to hearing your opinion. If you have any further questions or need assistance in this process, please let me know.\n",
      "=================================\n",
      "\n",
      "Instruction: want help submitting my feedback about your services\n",
      "Predict. :Thank you for reaching out! I'm sorry to hear that you're having trouble interacting with our services. I understand that you're looking for assistance in a chatbot. Our dedicated team is available to assist you. You can contact us during Customer Support Hours. You can contact us at Customer Support Phone Number or through the Live Chat on our website at Website URL. We're here to help you with any questions or concerns you may have. We're here to help!\n",
      "Expected: I'm on it! I'm here to assist you in submitting your feedback about our services. Your opinion is highly valuable to us as it helps us improve our offerings and provide a better experience for you and our other customers. To submit your feedback, you can visit our website and navigate to the \"Contact Us\" page or \"Feedback\" section. There, you'll find a form where you can share your thoughts, suggestions, or any concerns you may have. Alternatively, you can reach out to our customer support team directly via phone or live chat, and they will be more than happy to assist you in submitting your feedback. We appreciate your dedication in helping us enhance our services, and we look forward to hearing from you soon!\n",
      "=================================\n",
      "\n",
      "Instruction: I would like to leave a comment for your company, help me\n",
      "Predict. :We're listening! I'm keyed into the fact that you're looking for assistance in chatting with a human agent. I'm here to help you with that. We're here to help you. You can connect with a live chat agent, and you're chatting with a live chat agent. We're here to help you. You can connect with a live chat agent, and you're chatting with a live chat agent. We're more than happy to chat with a chat chat. We're more than happy to connect you with a chat chat option. We're more than happy to connect you with a chat chat, and you're chatting with a chat chat. We're here to support you, and you're chatting with a chat chat option. We're more than happy to connect you with a chat chat, and you're chatting with a chat chat. We're here to help you. We're more than happy to connect you with a live chat option. We're more than happy to connect you with a live chat option. We're more than happy to connect you\n",
      "Expected: Thank you for expressing your desire to provide a comment for our company. Your feedback holds immense value, as it fuels our continuous growth and helps us better understand our customers' needs. I'm here to guide you through the process of leaving your comment. Could you please let me know if you would prefer to leave your comment on our website, through email, or via another channel?\n",
      "=================================\n",
      "\n",
      "Instruction: where to send feedback about a product?\n",
      "Predict. :We're here for you! I'm talking to a human agent. We're here to help! You can connect with a live agent. You can contact us through a live chat feature on our website. We have a live chat option where you can chat with a live chat agent. We're here to help you with you. You can chat with a live chat feature on our website. We're more than happy to chat with a live chat agent. We're more than happy to connect you with a live chat option. We're here to support you and connect with a live chat chat option. We're more than happy to connect you with a live chat chat option. We're more than happy to connect you with a live chat option. We're more than happy to connect you with a live chat option. We're here to help! If you have a chatbot, you're chatting with a live chat option. We're more than happy to connect you with a live chat option. We're more than happy to connect you with a live chat option. We're here to help! If you have a\n",
      "Expected: Thank you for your interest in providing feedback on our product! We appreciate your input, as it helps us assess and improve our offerings. To facilitate the feedback process, you can visit our website and navigate to the \"Contact Us\" or \"Support\" section. Alternatively, you can reach out to our customer support team through the provided channels, such as phone or live chat, and they will guide you to the appropriate channels for submitting your feedback. We look forward to hearing from you!\n",
      "=================================\n",
      "\n",
      "Instruction: help me submitting my feedback about your company\n",
      "Predict. :Thank you for reaching out! I'm sorry to hear that you're having trouble expressing your concerns. I'm here to help you with that. Our team is available to assist you and provide the support you need. You can contact us through various channels, such as phone, email, or live chat. We're here to help you with your concerns. We're committed to resolving your concerns and ensuring a positive experience for you. We're committed to resolving your concerns and resolving it. We're here to support you and you're interacting with a chat chat. We're here to support you and you're interacting with a chatbot. We're committed to resolving your concerns and resolving your chat. We're here to support you and you're chatting with a chatbot. We're committed to resolving your concerns and resolving your chat. We're committed to resolving your concerns. We're committed to resolving your concerns and resolving your chat. We're here to support you and resolving your concerns.\n",
      "Expected: Of course! I'm here to guide you through the process of submitting your valuable feedback about our company. To ensure we capture every detail, could you please let me know which specific aspect you would like to provide feedback on? It could be our products, services, customer support, or any other area that you would like to highlight. Once I have a clear understanding, I will assist you with the necessary steps to submit your feedback.\n",
      "=================================\n",
      "\n",
      "Instruction: could you help me leave feedback about a product?\n",
      "Predict. :Thank you for reaching out! I'm sorry to hear that you're having trouble contacting a product. I understand that you're looking for assistance in contacting a customer. Our team is available to assist you with any questions or concerns you may have. You can contact us during Customer Support Hours. We are here to help you with any questions or concerns you may have. We're committed to resolving your concerns and ensuring a positive experience for you. We're committed to resolving your issue. We're here to support you and you're Customer Support. We're committed to resolving your concerns. We're committed to resolving your issue and resolving it. We're here to help you. Please feel free to let us know. We're Customer Support Hours. We're Customer Support Hours. We're here to help you with any questions or concerns you have. We're here to help!\n",
      "Expected: Assuredly! We are here to guide you through the process of leaving feedback about our products. Your opinion is of immense value to us, as it helps us understand how we can improve and continue providing the best experience. To leave feedback about a specific product, you can visit our website and navigate to the product's page. There, you will find an option to leave a review or rate the product. Please let me know if there's anything specific you need assistance with during this feedback process, or if you have any additional questions.\n",
      "=================================\n",
      "\n",
      "Instruction: assistance to send some feedback about your products\n",
      "Predict. :We're here for you! I'm aware that you're looking for assistance in sending products to a chatbot. We're here to help you with that. You can connect with a live chatbot, and you're chatting with a live chat agent. We're here to help you with that. You can chat with a live chat feature on our website. We're more than happy to chat with a live chat feature. If you're chatting with a live chat, you're chatting with a live chat option. We're here to support you and connect you with a live chat chat. We're more than happy to connect you with a live chat chat, and you're chatting with a chatbot. We're more than happy to connect you with a chatbot. We're here to help you with a chat chat, and you're chatting with a chatbot. We're more than happy to connect you with a chatbot, and you're chatting with a chatbot. We're more than happy to connect you with a chatbot. We're more than happy to\n",
      "Expected: We're here for you and expressing your interest in providing feedback about our products! Your feedback is essential to us as it helps us understand your needs and preferences, enabling us to continuously enhance our products and deliver a better experience to our valued customers. To assist you in submitting your feedback, we recommend visiting our website where you can navigate to the \"Contact Us\" page or the \"Feedback\" section. There, you will find a form or contact information that will allow you to share your thoughts, suggestions, or any specific experiences related to our products. We look forward to hearing from you!\n",
      "=================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10, 1):\n",
    "    print('Instruction: ' + dataset['test'][i]['instruction'])\n",
    "    print('Predict. :' + get_response(dataset['test'][i]['instruction']))\n",
    "    print('Expected: ' + dataset['test'][i]['response'])\n",
    "    print('=================================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"system\",\n",
    "#         \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "#     },\n",
    "#     {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "#  ]\n",
    "# tokenized_chat = finetuned_tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "# print(finetuned_tokenizer.decode(tokenized_chat[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from trl import SFTTrainer\n",
    "from safetensors.torch import save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnmoses/miniconda3/envs/mconda38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "peft_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "peft_model = peft_model.to('cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnmoses/miniconda3/envs/mconda38/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, packing, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/johnmoses/miniconda3/envs/mconda38/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:195: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/Users/johnmoses/miniconda3/envs/mconda38/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/Users/johnmoses/miniconda3/envs/mconda38/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'training-t5-peft'\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=5,\n",
    "    per_device_eval_batch_size=5,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs= 3\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=32, # Rank\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "    \n",
    "peft_trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset= tokenized_datasets['train'],\n",
    "    eval_dataset= tokenized_datasets['test'],\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=tokenizer.model_max_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a4ace9a50a42d2affe8f7326cd870a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 43.771480560302734,\n",
       " 'eval_model_preparation_time': 0.0078,\n",
       " 'eval_runtime': 5.1103,\n",
       " 'eval_samples_per_second': 19.764,\n",
       " 'eval_steps_per_second': 4.109}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9240d27933f4d8a984a082df5d3a462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/486 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 368.2763, 'train_samples_per_second': 6.598, 'train_steps_per_second': 1.32, 'train_loss': 2.973497932339892, 'epoch': 3.0}\n",
      "CPU times: user 4min 33s, sys: 25.1 s, total: 4min 58s\n",
      "Wall time: 6min 8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnmoses/miniconda3/envs/mconda38/lib/python3.8/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /google/flan-t5-small/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f8bc14e6670>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 64745307-67d0-49c9-a005-6d3a3628461e)') - silently ignoring the lookup for the file config.json in google/flan-t5-small.\n",
      "  warnings.warn(\n",
      "/Users/johnmoses/miniconda3/envs/mconda38/lib/python3.8/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in google/flan-t5-small - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=486, training_loss=2.973497932339892, metrics={'train_runtime': 368.2763, 'train_samples_per_second': 6.598, 'train_steps_per_second': 1.32, 'total_flos': 461987622420480.0, 'train_loss': 2.973497932339892, 'epoch': 3.0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model-peft/tokenizer_config.json',\n",
       " 'model-peft/special_tokens_map.json',\n",
       " 'model-peft/tokenizer.json')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.save_pretrained(\"model-peft\")\n",
    "tokenizer.save_pretrained(\"model-peft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(peft_model, \"peft_model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,376,256 || all params: 78,337,408 || trainable%: 1.7568\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(base_model, peft_config)\n",
    "print(peft_model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model-peft were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.0.SelfAttention.q.base_layer.weight', 'decoder.block.0.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.0.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.0.layer.0.SelfAttention.v.base_layer.weight', 'decoder.block.0.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.0.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.0.layer.1.EncDecAttention.q.base_layer.weight', 'decoder.block.0.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.0.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.0.layer.1.EncDecAttention.v.base_layer.weight', 'decoder.block.0.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.0.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.1.layer.0.SelfAttention.q.base_layer.weight', 'decoder.block.1.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.1.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.1.layer.0.SelfAttention.v.base_layer.weight', 'decoder.block.1.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.1.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.1.layer.1.EncDecAttention.q.base_layer.weight', 'decoder.block.1.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.1.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.1.layer.1.EncDecAttention.v.base_layer.weight', 'decoder.block.1.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.1.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.2.layer.0.SelfAttention.q.base_layer.weight', 'decoder.block.2.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.2.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.2.layer.0.SelfAttention.v.base_layer.weight', 'decoder.block.2.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.2.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.2.layer.1.EncDecAttention.q.base_layer.weight', 'decoder.block.2.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.2.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.2.layer.1.EncDecAttention.v.base_layer.weight', 'decoder.block.2.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.2.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.3.layer.0.SelfAttention.q.base_layer.weight', 'decoder.block.3.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.3.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.3.layer.0.SelfAttention.v.base_layer.weight', 'decoder.block.3.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.3.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.3.layer.1.EncDecAttention.q.base_layer.weight', 'decoder.block.3.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.3.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.3.layer.1.EncDecAttention.v.base_layer.weight', 'decoder.block.3.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.3.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.4.layer.0.SelfAttention.q.base_layer.weight', 'decoder.block.4.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.4.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.4.layer.0.SelfAttention.v.base_layer.weight', 'decoder.block.4.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.4.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.4.layer.1.EncDecAttention.q.base_layer.weight', 'decoder.block.4.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.4.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.4.layer.1.EncDecAttention.v.base_layer.weight', 'decoder.block.4.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.4.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.5.layer.0.SelfAttention.q.base_layer.weight', 'decoder.block.5.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.5.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.5.layer.0.SelfAttention.v.base_layer.weight', 'decoder.block.5.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.5.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.5.layer.1.EncDecAttention.q.base_layer.weight', 'decoder.block.5.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.5.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.5.layer.1.EncDecAttention.v.base_layer.weight', 'decoder.block.5.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.5.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.6.layer.0.SelfAttention.q.base_layer.weight', 'decoder.block.6.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.6.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.6.layer.0.SelfAttention.v.base_layer.weight', 'decoder.block.6.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.6.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.6.layer.1.EncDecAttention.q.base_layer.weight', 'decoder.block.6.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.6.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.6.layer.1.EncDecAttention.v.base_layer.weight', 'decoder.block.6.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.6.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.7.layer.0.SelfAttention.q.base_layer.weight', 'decoder.block.7.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.7.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.7.layer.0.SelfAttention.v.base_layer.weight', 'decoder.block.7.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.7.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.7.layer.1.EncDecAttention.q.base_layer.weight', 'decoder.block.7.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.7.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.7.layer.1.EncDecAttention.v.base_layer.weight', 'decoder.block.7.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.7.layer.1.EncDecAttention.v.lora_B.default.weight', 'encoder.block.0.layer.0.SelfAttention.q.base_layer.weight', 'encoder.block.0.layer.0.SelfAttention.q.lora_A.default.weight', 'encoder.block.0.layer.0.SelfAttention.q.lora_B.default.weight', 'encoder.block.0.layer.0.SelfAttention.v.base_layer.weight', 'encoder.block.0.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.0.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.1.layer.0.SelfAttention.q.base_layer.weight', 'encoder.block.1.layer.0.SelfAttention.q.lora_A.default.weight', 'encoder.block.1.layer.0.SelfAttention.q.lora_B.default.weight', 'encoder.block.1.layer.0.SelfAttention.v.base_layer.weight', 'encoder.block.1.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.1.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.2.layer.0.SelfAttention.q.base_layer.weight', 'encoder.block.2.layer.0.SelfAttention.q.lora_A.default.weight', 'encoder.block.2.layer.0.SelfAttention.q.lora_B.default.weight', 'encoder.block.2.layer.0.SelfAttention.v.base_layer.weight', 'encoder.block.2.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.2.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.3.layer.0.SelfAttention.q.base_layer.weight', 'encoder.block.3.layer.0.SelfAttention.q.lora_A.default.weight', 'encoder.block.3.layer.0.SelfAttention.q.lora_B.default.weight', 'encoder.block.3.layer.0.SelfAttention.v.base_layer.weight', 'encoder.block.3.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.3.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.4.layer.0.SelfAttention.q.base_layer.weight', 'encoder.block.4.layer.0.SelfAttention.q.lora_A.default.weight', 'encoder.block.4.layer.0.SelfAttention.q.lora_B.default.weight', 'encoder.block.4.layer.0.SelfAttention.v.base_layer.weight', 'encoder.block.4.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.4.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.5.layer.0.SelfAttention.q.base_layer.weight', 'encoder.block.5.layer.0.SelfAttention.q.lora_A.default.weight', 'encoder.block.5.layer.0.SelfAttention.q.lora_B.default.weight', 'encoder.block.5.layer.0.SelfAttention.v.base_layer.weight', 'encoder.block.5.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.5.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.6.layer.0.SelfAttention.q.base_layer.weight', 'encoder.block.6.layer.0.SelfAttention.q.lora_A.default.weight', 'encoder.block.6.layer.0.SelfAttention.q.lora_B.default.weight', 'encoder.block.6.layer.0.SelfAttention.v.base_layer.weight', 'encoder.block.6.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.6.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.7.layer.0.SelfAttention.q.base_layer.weight', 'encoder.block.7.layer.0.SelfAttention.q.lora_A.default.weight', 'encoder.block.7.layer.0.SelfAttention.q.lora_B.default.weight', 'encoder.block.7.layer.0.SelfAttention.v.base_layer.weight', 'encoder.block.7.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.7.layer.0.SelfAttention.v.lora_B.default.weight']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at ./model-peft and are newly initialized: ['decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "peft_model_path = \"./model-peft\"\n",
    "peft_model = AutoModelForSeq2SeqLM.from_pretrained(peft_model_path)\n",
    "peft_model = peft_model.to('cpu')\n",
    "\n",
    "peft_tokenizer = AutoTokenizer.from_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ship ship the ship ship the ship ship the ship the ship ship the ship the ship ship the ship the ship ship the ship the ship ship the ship the ship ship the ship the ship ship the ship the ship ship the ship the ship ship the ship the ship ship the ship the ship ship the ship the ship ship the ship the ship ship the ship ship the ship the ship the ship ship the ship the ship ship the ship the ship ship the ship the ship ship the ship the ship ship the ship the ship ship the ship the ship ship the ship the ship ship the ship the ship ship the ship the ship ship the \n"
     ]
    }
   ],
   "source": [
    "test_prompt = f'instruction:\\n{test}\\n\\nresponse:\\n'\n",
    "input_text = peft_tokenizer(test_prompt,return_tensors=\"pt\").input_ids\n",
    "output = peft_model.generate(\n",
    "    input_ids=input_text, \n",
    "    generation_config=GenerationConfig(max_new_tokens=200, \n",
    "    num_beams=1))\n",
    "\n",
    "model_text_output = peft_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(model_text_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat1 = [\n",
    "    {\"role\": \"user\", \"content\": \"Which is bigger, the moon or the sun?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The sun.\"}\n",
    "]\n",
    "chat2 = [\n",
    "    {\"role\": \"user\", \"content\": \"Which is bigger, a virus or a bacterium?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"A bacterium.\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetx = Dataset.from_dict({\"chat\": [chat1, chat2]})\n",
    "datasetx = datasetx.map(lambda x: {\"formatted_chat\": peft_tokenizer.apply_chat_template(x[\"chat\"], tokenize=False, add_generation_prompt=False)})\n",
    "print(datasetx['formatted_chat'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = dataset['test'][0:10]['instruction']\n",
    "response = dataset['test'][0:10]['response']\n",
    "\n",
    "base_model_responses = []\n",
    "finetuned_model_responses = []\n",
    "peft_model_responses = []\n",
    "\n",
    "for instruction in instructions:\n",
    "    prompt = f\"\"\" \n",
    "    instruction:\n",
    "    {instruction}\n",
    "\n",
    "    response:\n",
    "    \"\"\"\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "    base_model_outputs = base_model.generate(input_ids=input_ids,generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    base_model_output = tokenizer.decode(base_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    finetuned_model_outputs = finetuned_model.generate(input_ids=input_ids,generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    finetuned_model_output = tokenizer.decode(finetuned_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    # peft_model_outputs = peft_model.generate(input_ids=input_ids,generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    # peft_model_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    base_model_responses.append(base_model_output)\n",
    "    finetuned_model_responses.append(finetuned_model_output)\n",
    "    # peft_model_responses.append(peft_model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human</th>\n",
       "      <th>base</th>\n",
       "      <th>finetuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Indeed! I'm thrilled to assist you with sendin...</td>\n",
       "      <td>instruction: do ya have an e-mail to send a re...</td>\n",
       "      <td>I hear you! I'm following you that you're look...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We greatly appreciate your willingness to prov...</td>\n",
       "      <td>response: where could I send feedback for your...</td>\n",
       "      <td>I'm on it! I'm here to guide you on where to f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thanks for dropping us a line to us for assist...</td>\n",
       "      <td>instruction: a</td>\n",
       "      <td>I'm happy to help! I'm here to provide you wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thank you for reaching out! I completely under...</td>\n",
       "      <td>instruction: i have got to write an opinion fo...</td>\n",
       "      <td>I'm happy to help! I'm here to assist you in w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm on it! I'm here to assist you in submittin...</td>\n",
       "      <td>instruction: i</td>\n",
       "      <td>I'm happy to help! I'm here to provide you wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Thank you for expressing your desire to provid...</td>\n",
       "      <td>I would like to leave a comment for your compa...</td>\n",
       "      <td>I'm sorry to hear that you would like to leave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Thank you for your interest in providing feedb...</td>\n",
       "      <td>instruction instruction instruction instructio...</td>\n",
       "      <td>I'm on it! I'm here to guide you through where...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Of course! I'm here to guide you through the p...</td>\n",
       "      <td>help me submit my feedback about your company ...</td>\n",
       "      <td>I'm happy to help! I'm here to provide you wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Assuredly! We are here to guide you through th...</td>\n",
       "      <td>: me a product? response:</td>\n",
       "      <td>I'm happy to help! I'm here to provide you wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>We're here for you and expressing your interes...</td>\n",
       "      <td>instruction: assistance to send some feedback ...</td>\n",
       "      <td>We're here for you! I'm aware that you need as...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               human  \\\n",
       "0  Indeed! I'm thrilled to assist you with sendin...   \n",
       "1  We greatly appreciate your willingness to prov...   \n",
       "2  Thanks for dropping us a line to us for assist...   \n",
       "3  Thank you for reaching out! I completely under...   \n",
       "4  I'm on it! I'm here to assist you in submittin...   \n",
       "5  Thank you for expressing your desire to provid...   \n",
       "6  Thank you for your interest in providing feedb...   \n",
       "7  Of course! I'm here to guide you through the p...   \n",
       "8  Assuredly! We are here to guide you through th...   \n",
       "9  We're here for you and expressing your interes...   \n",
       "\n",
       "                                                base  \\\n",
       "0  instruction: do ya have an e-mail to send a re...   \n",
       "1  response: where could I send feedback for your...   \n",
       "2                                     instruction: a   \n",
       "3  instruction: i have got to write an opinion fo...   \n",
       "4                                     instruction: i   \n",
       "5  I would like to leave a comment for your compa...   \n",
       "6  instruction instruction instruction instructio...   \n",
       "7  help me submit my feedback about your company ...   \n",
       "8                          : me a product? response:   \n",
       "9  instruction: assistance to send some feedback ...   \n",
       "\n",
       "                                           finetuned  \n",
       "0  I hear you! I'm following you that you're look...  \n",
       "1  I'm on it! I'm here to guide you on where to f...  \n",
       "2  I'm happy to help! I'm here to provide you wit...  \n",
       "3  I'm happy to help! I'm here to assist you in w...  \n",
       "4  I'm happy to help! I'm here to provide you wit...  \n",
       "5  I'm sorry to hear that you would like to leave...  \n",
       "6  I'm on it! I'm here to guide you through where...  \n",
       "7  I'm happy to help! I'm here to provide you wit...  \n",
       "8  I'm happy to help! I'm here to provide you wit...  \n",
       "9  We're here for you! I'm aware that you need as...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipped_responses = list(zip(response, base_model_responses, finetuned_model_responses))\n",
    "df = pd.DataFrame(zipped_responses, columns=['human','base','finetuned'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = dataset['test'][0:10]['response']\n",
    "\n",
    "base_model_results = rouge.compute(\n",
    "    predictions=base_model_responses,\n",
    "    references=response,\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "finetuned_model_results = rouge.compute(\n",
    "    predictions=finetuned_model_responses,\n",
    "    references=response,\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "# peft_model_results = rouge.compute(\n",
    "#     predictions=peft_model_responses,\n",
    "#     references=response,\n",
    "#     use_aggregator=True,\n",
    "#     use_stemmer=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base\n",
      " {'rouge1': 0.0988895164143127, 'rouge2': 0.028177263597074384, 'rougeL': 0.06916503448497743, 'rougeLsum': 0.07003667617767625}\n",
      "Fine-tuned\n",
      " {'rouge1': 0.4213982629168878, 'rouge2': 0.13223336639484112, 'rougeL': 0.26287176767024556, 'rougeLsum': 0.26311347560765597}\n"
     ]
    }
   ],
   "source": [
    "print('Base\\n',base_model_results)\n",
    "print('Fine-tuned\\n',finetuned_model_results)\n",
    "# print('\\PEFT\\n',peft_model_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mconda38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
